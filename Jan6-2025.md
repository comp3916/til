Using EF core with Sql functions (specifically JSON related in this example)
```
protected override void OnModelCreating(ModelBuilder modelBuilder)
{
    modelBuilder.HasDbFunction(
        typeof(Database).GetMethod(nameof(JsonValue))!
    ).HasName("JSON_VALUE").IsBuiltIn();
    
    modelBuilder.HasDbFunction(
        typeof(Database).GetMethod(nameof(JsonQuery))!
    ).HasName("JSON_QUERY").IsBuiltIn();
}

public static string JsonValue(string column, [NotParameterized] string path)
    => throw new NotSupportedException();

public static string JsonQuery(string column, [NotParameterized] string path) => 
    throw new NotSupportedException();
```


```
public class Product
{
    public int Id { get; set; }
    // serialized ProductInfo
    public string Json { get; set; }
        = "{ }";
}

public class ProductInfo
{
    public string Name { get; set; } = "";
    public decimal Price { get; set; }
}


using Microsoft.EntityFrameworkCore;

// project namespace
using SqlServerJson;
using static SqlServerJson.Database;
using PI = SqlServerJson.ProductInfo;

var database = new Database().TrySeed();

var expensive = database.Products
        .Select(p => new {
            p.Id,
            Name = JsonValue(p.Json, $"$.{nameof(PI.Name)}"),
            Price = Convert.ToDecimal(JsonValue(p.Json, $"$.{nameof(PI.Price)}"))
        })
        .Where(x => x.Price > 800)
        .OrderByDescending(x => x.Price)
        .Take(10);

Console.WriteLine(expensive.ToQueryString() + "\n");

foreach (var product in expensive)
{
    Console.WriteLine($"{product.Name} ({product.Price:C})");
}


```

## Tensor space
Tensor space is different from the typical multidimensional space used in explaining linear regression models. While both involve multiple dimensions, they have distinct characteristics and applications.

## Key Differences

### Multidimensional Space in Linear Regression

In linear regression, we typically work with a multidimensional space where each dimension represents a feature or predictor variable. For instance, in a multiple linear regression model with two predictor variables, we can visualize this as a three-dimensional space (two dimensions for predictors, one for the response variable)[2].

The equation for such a model might look like:
```
y = β₀ + β₁x₁ + β₂x₂ + ε
```
Where y is the response variable, x₁ and x₂ are predictor variables, β₀, β₁, and β₂ are coefficients, and ε is the error term.

### Tensor Space

Tensor space, on the other hand, is a more complex mathematical construct. A tensor is a multilinear map from a product of vector spaces to another vector space[1]. Tensors can be thought of as generalizations of scalars (0-order tensors), vectors (1st-order tensors), and matrices (2nd-order tensors) to higher orders[4].

## Example to Illustrate the Difference

Let's consider a practical example to highlight the distinction:

1. **Linear Regression Space**: 
Imagine we're predicting house prices based on square footage and number of bedrooms. Our space would be 3-dimensional: x-axis for square footage, y-axis for number of bedrooms, and z-axis for price. Each point in this space represents a house, and we're fitting a plane through these points.

2. **Tensor Space**:
Now, consider analyzing the stress in a material. We might use a stress tensor, which is a 2nd-order tensor. This tensor doesn't just represent points in space, but rather describes how forces act on surfaces in different directions. It captures relationships between multiple vectors and can be represented as a 3x3 matrix, where each element describes stress in a particular direction[3].

## Key Distinctions

1. **Transformation Properties**: Tensors have specific transformation rules when changing coordinate systems, which is crucial in physics and engineering[7].

2. **Multilinear Relationships**: Tensors can describe complex multilinear relationships that go beyond the linear combinations in typical regression models[1][4].

3. **Geometric Meaning**: Tensors have intrinsic geometric meanings, often representing physical quantities or operations, rather than just points in a feature space[7].

4. **Dimensionality**: While regression spaces typically deal with a fixed number of dimensions, tensor spaces can work with arbitrary numbers of dimensions and complex interactions between them[4].

In summary, while both concepts involve multiple dimensions, tensor spaces offer a more sophisticated framework for describing complex, multilinear relationships and physical phenomena, going beyond the simpler feature-response relationships modeled in typical linear regression spaces[1][4][7].

Citations:

1 https://www.reddit.com/r/math/comments/5evch3/eliapexplain_like_im_a_physicist_what_is_the/
2 https://datascience.stackexchange.com/questions/80854/imagining-a-linear-regression-model-with-more-than-3-dimensions
3 https://physics.stackexchange.com/questions/20437/are-matrices-and-second-rank-tensors-the-same-thing
4 https://en.wikipedia.org/wiki/Classical_treatment_of_tensors
5 https://www.youtube.com/watch?v=jXVvsa58aoQ
6 https://bjlkeng.io/posts/tensors-tensors-tensors/
7 https://news.ycombinator.com/item?id=9506774
8 https://www.mathworks.com/help/stats/multivariate-regression-1.html
9 https://en.wikipedia.org/wiki/Linear_regression_model



A vector space is like a playground for mathematical objects called vectors. Think of vectors as arrows that have both direction and length.

Here are the key ideas:

1. Collection of vectors: A vector space is a set of vectors that you can add together and multiply by numbers (called scalars).

2. Addition: You can add vectors together, just like you can add numbers. When you add two vectors, you get another vector that's still in the same space.

3. Scalar multiplication: You can multiply a vector by a number (scalar). This changes the vector's length but keeps it in the same space.

4. Zero vector: There's a special vector called the zero vector. Adding it to any other vector doesn't change anything, just like adding 0 to a number.

5. Rules: Vector spaces follow certain rules, like:
   - Adding vectors is commutative: A + B = B + A
   - Scalar multiplication is distributive: 2(A + B) = 2A + 2B

An example you might be familiar with is the 2D coordinate plane. Each point on this plane can be represented by a vector from the origin (0,0) to that point. All these vectors together form a vector space.

In summary, a vector space is a mathematical structure where you can add vectors and multiply them by numbers, following specific rules. It's a fundamental concept in higher math and has many applications in physics, engineering, and computer science.


Vector spaces, while abstract mathematical concepts, have numerous real-life applications. Here are some interesting examples of vector spaces that occur in various fields:

## Scientific and Engineering Applications

1. **Chemical Compositions**: The space of chemical compounds can be represented as a vector space, where each component represents the amount of a particular element or molecule[5].

2. **Text Analysis**: Documents can be represented as vectors in a high-dimensional space, where each dimension corresponds to a word in the vocabulary. This is useful in natural language processing and information retrieval[5].

3. **Image Processing**: The space of n × m real matrices can represent digital images, where each entry indicates the brightness of a pixel. This allows for image compression and analysis techniques[2].

4. **Signal Processing**: The space of continuous functions on an interval [a,b] is used to represent signals in various applications[2].

## Mathematical and Computational Examples

1. **Polynomial Spaces**: The set of polynomials of degree less than or equal to n forms a vector space. This has applications in interpolation and spline functions[2].

2. **Sequence Spaces**: The space of sequences satisfying linear recurrence relations, such as the Fibonacci sequence, can be analyzed as a vector space[2].

3. **Function Spaces**: The set of differentiable functions forms a vector space, allowing for analysis of complex mathematical relationships[1].

4. **Matrix Spaces**: The set of all n × m matrices forms a vector space, which is crucial in linear algebra and its applications[3].

## Other Interesting Examples

1. **Color Spaces**: RGB color models can be represented as a three-dimensional vector space.

2. **Financial Modeling**: Portfolio theory often uses vector spaces to represent different investment options.

3. **Quantum Mechanics**: State vectors in quantum mechanics form a complex vector space.

These examples demonstrate how vector spaces provide a powerful framework for modeling and analyzing various real-world phenomena across different disciplines.

Citations:
1 https://math.libretexts.org/Bookshelves/Linear_Algebra/Map:_Linear_Algebra_(Waldron_Cherney_and_Denton)/05:_Vector_Spaces/5.01:_Examples_of_Vector_Spaces
2 https://matheducators.stackexchange.com/questions/21048/big-list-of-interesting-abstract-vector-spaces
3 https://en.wikipedia.org/wiki/Polynomial_vector_spaces
4 https://www.youtube.com/watch?v=mU7DHh6KNzI
5 https://bvanderlei.github.io/jupyter-guide-to-linear-algebra/Applications_VS.html



