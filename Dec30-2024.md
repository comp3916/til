Check out this book for discussing Nifi in K8s environment

![image](https://github.com/user-attachments/assets/0a7b73dc-a63c-4de8-bd1d-2a15700e566c)

![image](https://github.com/user-attachments/assets/057be4c4-3af9-4101-a6da-a081b8a30ee2)

![image](https://github.com/user-attachments/assets/c6b5e56c-5dd1-468c-9006-db406aab91a2)

GitOps is a process popularized by Weaveworks. GitOps involves the use of applications reacting to git push events. GitOps focuses primarily on Kubernetes clusters matching the state described by configuration residing in a Git repository. On a simplistic level, GitOps aims to replace kubectl apply with git push.



![image](https://github.com/user-attachments/assets/4cd3f985-5277-4485-b1ae-f50ca38e9a56)

![image](https://github.com/user-attachments/assets/bf89ea3d-4126-4042-a78a-b8209532d48b)

![image](https://github.com/user-attachments/assets/7e115865-aa8f-4c49-9eae-d281ad35b01c)

![image](https://github.com/user-attachments/assets/53cda1d4-bcfd-40fc-a481-ab44d2259b0d)

![image](https://github.com/user-attachments/assets/17782e98-e431-480d-b3bd-f4aae25e90bf)


Apache Hive can effectively support configurations where data is stored in cloud storage. 



* **Cloud Storage as Data Lake:** Cloud storage services like Amazon S3, Google Cloud Storage (GCS), and Azure Blob Storage can serve as the underlying data lake for Hive. Hive can directly read and write data from these cloud storage locations.

* **Integration:** Hive integrates with cloud storage through mechanisms like:
    * **Storage Handlers:** These provide custom connectors for Hive to interact with specific cloud storage services.
    * **File Systems:** Hive can be configured to treat cloud storage as a file system, allowing it to access data using familiar file system paths.

* **Benefits:**
    * **Scalability and Cost-Effectiveness:** Cloud storage offers virtually unlimited scalability and cost-effective storage for massive datasets.
    * **Data Durability and Availability:** Cloud providers offer high levels of data durability and availability with features like data replication and redundancy.
    * **Integration with Cloud Services:** Seamlessly integrate with other cloud services like data pipelines, analytics tools, and machine learning platforms.

**In Summary:**

Hive's ability to work with cloud storage makes it a powerful tool for building data warehouses and performing data analytics on large-scale datasets in the cloud. 

**Key Considerations:**

* **Performance:** Network latency can impact query performance when working with data in cloud storage. Consider optimization techniques like data partitioning and caching to mitigate this.
* **Cost Optimization:** Understand the cost implications of storing and processing data in cloud storage. Utilize features like storage classes and data lifecycle management to optimize costs.

By leveraging the power of cloud storage, Hive can provide a flexible and scalable solution for your data warehousing and analytics needs.

While HDFS itself is designed to be a distributed file system running on a cluster of machines, there are ways to leverage cloud storage in conjunction with HDFS:

* **Cloud Storage as a Tier:** 
    * You can use cloud storage as a cold storage tier for infrequently accessed data. 
    * HDFS can be configured to automatically archive older or less frequently used data to cloud storage. 
    * This helps reduce the storage costs associated with maintaining large datasets on the HDFS cluster.

* **Cloud Storage for Data Ingestion/Egress:** 
    * Cloud storage can be used as a staging area for data before it's loaded into HDFS. 
    * Data can be uploaded to cloud storage from various sources, and then moved to HDFS for processing. 
    * Similarly, processed data from HDFS can be moved to cloud storage for long-term archival or for sharing with other applications.

* **Hybrid Architectures:** Some organizations implement hybrid architectures where they have an on-premises HDFS cluster and leverage cloud storage for certain use cases, such as:
    * Disaster recovery: Replicate HDFS data to cloud storage for backup and disaster recovery purposes.
    * Data sharing: Share data with external partners or collaborators by storing it in cloud storage.

**Important Considerations:**

* **Data Movement:** Moving large amounts of data between HDFS and cloud storage can be time-consuming and expensive. 
* **Data Consistency:** Maintaining data consistency between HDFS and cloud storage requires careful planning and implementation.
* **Cost Optimization:** Carefully evaluate the costs associated with storing and transferring data to and from cloud storage.

**In Summary:**

While HDFS itself doesn't directly "leverage" cloud storage in the same way that Hive does, they can work together effectively in a hybrid cloud environment. By strategically utilizing cloud storage for data archival, ingestion/egress, and other use cases, organizations can optimize their data storage and processing infrastructure.



**Rust Notes**
=============
![image](https://github.com/user-attachments/assets/1ea82085-06ea-4e07-b481-d579a1592d3d)

![image](https://github.com/user-attachments/assets/0e2b8024-6886-4d82-8a61-b3b35edda26d)

**Go Notes**
===============

Came across a naming convention that I found to be useful. Assume you have a folder kworkspace. Create two directions - one below the other in the kworkspace directories. The names of the directories will match the go mod init command
Eg: Create the directory apk8s/compressor. Inititialize the go mod from inside the compressor folder. 
```
go mod init github.com/apk8s/compressor
```

Create a cmd folder since we are creating a command line pgm. Create a file compressor.go in the cmd directory as shown

![image](https://github.com/user-attachments/assets/d7519aa3-1b10-4804-add0-a525aabfb2c0)

To execute the program
```
go run ./cmd/compressor.go
```

Create the Dockerfile in compressor folder as shown

![image](https://github.com/user-attachments/assets/e35cebb0-07df-4de5-b65e-08f10f5a03e4)














